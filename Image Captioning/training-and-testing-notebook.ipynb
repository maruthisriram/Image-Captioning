{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Importing Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom numpy import array\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport string\nimport os\nimport glob\nfrom PIL import Image\nfrom time import time\n\nfrom keras import Input, layers\nfrom keras import optimizers\nfrom tensorflow.keras.optimizers import Adam\nfrom keras.preprocessing import sequence\nfrom keras.preprocessing import image\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import LSTM, Embedding, Dense, Activation, Flatten, Reshape, Dropout\nfrom keras.layers.wrappers import Bidirectional\nfrom keras.layers.merge import add\nfrom keras.applications.inception_v3 import InceptionV3\nfrom keras.applications.inception_v3 import preprocess_input\nfrom keras.models import Model\nfrom tensorflow.keras.utils import to_categorical","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-08T07:08:15.050559Z","iopub.execute_input":"2022-05-08T07:08:15.05118Z","iopub.status.idle":"2022-05-08T07:08:20.710055Z","shell.execute_reply.started":"2022-05-08T07:08:15.051086Z","shell.execute_reply":"2022-05-08T07:08:20.709187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Finding the Paths to the dataset","metadata":{}},{"cell_type":"code","source":"tokens_path = \"../input/flickr8k-text/Flickr8k_text/Flickr8k.token.txt\"\ntrain_images_path = '../input/flickr8k-text/Flickr8k_text/Flickr_8k.trainImages.txt'\ntest_images_path = '../input/flickr8k-text/Flickr8k_text/Flickr_8k.testImages.txt'\nimages_path = '../input/flickr8k-images/Flicker8k_Images/'\nglove_path = '../input/glove6b'\n\ndoc = open(tokens_path,'r').read()\nprint(doc[:410])","metadata":{"execution":{"iopub.status.busy":"2022-05-08T07:08:20.711734Z","iopub.execute_input":"2022-05-08T07:08:20.712477Z","iopub.status.idle":"2022-05-08T07:08:20.766062Z","shell.execute_reply.started":"2022-05-08T07:08:20.712443Z","shell.execute_reply":"2022-05-08T07:08:20.765237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Creating the dictionary of images and their captions","metadata":{}},{"cell_type":"code","source":"descriptions = dict()\nfor line in doc.split('\\n'):\n        tokens = line.split()\n        if len(line) > 2:\n          image_id = tokens[0].split('.')[0]\n          image_desc = ' '.join(tokens[1:])\n          if image_id not in descriptions:\n              descriptions[image_id] = list()\n          descriptions[image_id].append(image_desc)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T07:08:20.768917Z","iopub.execute_input":"2022-05-08T07:08:20.769305Z","iopub.status.idle":"2022-05-08T07:08:20.933693Z","shell.execute_reply.started":"2022-05-08T07:08:20.769258Z","shell.execute_reply":"2022-05-08T07:08:20.930576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tidy up the captions by removing the punctuations, convert to lower case and split captions at spaces","metadata":{}},{"cell_type":"code","source":"table = str.maketrans('', '', string.punctuation)\nfor key, desc_list in descriptions.items():\n    for i in range(len(desc_list)):\n        desc = desc_list[i]\n        desc = desc.split()\n        desc = [word.lower() for word in desc]\n        desc = [w.translate(table) for w in desc]\n        desc_list[i] =  ' '.join(desc)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T07:08:20.936204Z","iopub.execute_input":"2022-05-08T07:08:20.936719Z","iopub.status.idle":"2022-05-08T07:08:21.507171Z","shell.execute_reply.started":"2022-05-08T07:08:20.936649Z","shell.execute_reply":"2022-05-08T07:08:21.502757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Showing one of the image and its captions","metadata":{}},{"cell_type":"code","source":"pic = '2398605966_1d0c9e6a20.jpg'\nx=plt.imread(images_path+pic)\nplt.imshow(x)\nplt.show()\ndescriptions['2398605966_1d0c9e6a20']","metadata":{"execution":{"iopub.status.busy":"2022-05-08T07:08:21.511618Z","iopub.execute_input":"2022-05-08T07:08:21.51366Z","iopub.status.idle":"2022-05-08T07:08:21.840491Z","shell.execute_reply.started":"2022-05-08T07:08:21.513614Z","shell.execute_reply":"2022-05-08T07:08:21.839766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Create a vocabulary by taking only unique words","metadata":{}},{"cell_type":"code","source":"vocabulary = set()\nfor key in descriptions.keys():\n        [vocabulary.update(d.split()) for d in descriptions[key]]\nprint('Original Vocabulary Size: %d' % len(vocabulary))","metadata":{"execution":{"iopub.status.busy":"2022-05-08T07:08:21.844305Z","iopub.execute_input":"2022-05-08T07:08:21.844813Z","iopub.status.idle":"2022-05-08T07:08:21.954899Z","shell.execute_reply.started":"2022-05-08T07:08:21.844774Z","shell.execute_reply":"2022-05-08T07:08:21.954135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lines = list()\nfor key, desc_list in descriptions.items():\n    for desc in desc_list:\n        lines.append(key + ' ' + desc)\nnew_descriptions = '\\n'.join(lines)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T07:08:21.959105Z","iopub.execute_input":"2022-05-08T07:08:21.961018Z","iopub.status.idle":"2022-05-08T07:08:21.998702Z","shell.execute_reply.started":"2022-05-08T07:08:21.960979Z","shell.execute_reply":"2022-05-08T07:08:21.997837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Read the training images and the captions and create a training dataset\n### Do the same process and create a test dataset","metadata":{}},{"cell_type":"code","source":"doc = open(train_images_path,'r').read()\ndataset_train = list()\nfor line in doc.split('\\n'):\n    if len(line) > 1:\n      identifier = line.split('.')[0]\n      dataset_train.append(identifier)\n\ntrain = set(dataset_train)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T07:08:22.003236Z","iopub.execute_input":"2022-05-08T07:08:22.005204Z","iopub.status.idle":"2022-05-08T07:08:22.028837Z","shell.execute_reply.started":"2022-05-08T07:08:22.005161Z","shell.execute_reply":"2022-05-08T07:08:22.027983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"doc = open(test_images_path,'r').read()\ndataset_test = list()\nfor line in doc.split('\\n'):\n    if len(line) > 1:\n      identifier = line.split('.')[0]\n      dataset_test.append(identifier)\n\ntest = set(dataset_test)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T07:08:22.03323Z","iopub.execute_input":"2022-05-08T07:08:22.035231Z","iopub.status.idle":"2022-05-08T07:08:22.062284Z","shell.execute_reply.started":"2022-05-08T07:08:22.035189Z","shell.execute_reply":"2022-05-08T07:08:22.061305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img = glob.glob(images_path + '*.jpg')\ntrain_images = set(open(train_images_path, 'r').read().strip().split('\\n'))\ntrain_img = []\nfor i in img: \n    if i[len(images_path):] in train_images:\n        train_img.append(i)\n\ntest_images = set(open(test_images_path, 'r').read().strip().split('\\n'))\ntest_img = []\nfor i in img: \n    if i[len(images_path):] in test_images: \n        test_img.append(i)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T07:08:22.065337Z","iopub.execute_input":"2022-05-08T07:08:22.068847Z","iopub.status.idle":"2022-05-08T07:08:22.32344Z","shell.execute_reply.started":"2022-05-08T07:08:22.068799Z","shell.execute_reply":"2022-05-08T07:08:22.322704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Create a dictionary of Train Descriptions with mappings from training images id to all its captions\n#### We append startseq and endseq at the start and end of the captions to create 2 sentences and feed it to the model\n","metadata":{}},{"cell_type":"code","source":"train_descriptions = dict()\nfor line in new_descriptions.split('\\n'):\n    tokens = line.split()\n    image_id, image_desc = tokens[0], tokens[1:]\n    if image_id in train:\n        if image_id not in train_descriptions:\n            train_descriptions[image_id] = list()\n        desc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n        train_descriptions[image_id].append(desc)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T07:08:22.325475Z","iopub.execute_input":"2022-05-08T07:08:22.325905Z","iopub.status.idle":"2022-05-08T07:08:22.428261Z","shell.execute_reply.started":"2022-05-08T07:08:22.325869Z","shell.execute_reply":"2022-05-08T07:08:22.427509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Do the same for the test descriptions","metadata":{}},{"cell_type":"code","source":"test_descriptions = dict()\nfor line in new_descriptions.split('\\n'):\n    tokens = line.split()\n    image_id, image_desc = tokens[0], tokens[1:]\n    if image_id in test:\n        if image_id not in test_descriptions:\n            test_descriptions[image_id] = list()\n        desc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n        test_descriptions[image_id].append(desc)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T07:08:22.430496Z","iopub.execute_input":"2022-05-08T07:08:22.430946Z","iopub.status.idle":"2022-05-08T07:08:22.497516Z","shell.execute_reply.started":"2022-05-08T07:08:22.430898Z","shell.execute_reply":"2022-05-08T07:08:22.496654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_train_captions = []\nfor key, val in train_descriptions.items():\n    for cap in val:\n        all_train_captions.append(cap)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T07:08:22.499599Z","iopub.execute_input":"2022-05-08T07:08:22.500065Z","iopub.status.idle":"2022-05-08T07:08:22.510325Z","shell.execute_reply.started":"2022-05-08T07:08:22.500026Z","shell.execute_reply":"2022-05-08T07:08:22.509672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Take only those words in the captions which occur more than 10 times in the whole dataset","metadata":{}},{"cell_type":"code","source":"word_limit = 10\nword_counts = {}\nnsents = 0\nfor sent in all_train_captions:\n    nsents += 1\n    for w in sent.split(' '):\n        word_counts[w] = word_counts.get(w, 0) + 1\nvocab = [w for w in word_counts if word_counts[w] >= word_limit]\n\nprint('Vocabulary = %d' % (len(vocab)))","metadata":{"execution":{"iopub.status.busy":"2022-05-08T07:08:22.512683Z","iopub.execute_input":"2022-05-08T07:08:22.513124Z","iopub.status.idle":"2022-05-08T07:08:22.668022Z","shell.execute_reply.started":"2022-05-08T07:08:22.513095Z","shell.execute_reply":"2022-05-08T07:08:22.667311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Create 2 mappings from index to word and word to index","metadata":{}},{"cell_type":"code","source":"idx_to_word = {}\nword_to_idx = {}\nix = 1\nfor w in vocab:\n    word_to_idx[w] = ix\n    idx_to_word[ix] = w\n    ix += 1\n\nvocab_size = len(idx_to_word) + 1","metadata":{"execution":{"iopub.status.busy":"2022-05-08T07:08:22.669409Z","iopub.execute_input":"2022-05-08T07:08:22.669715Z","iopub.status.idle":"2022-05-08T07:08:22.675126Z","shell.execute_reply.started":"2022-05-08T07:08:22.669666Z","shell.execute_reply":"2022-05-08T07:08:22.674379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### We find the description with maximum length and make all the other descriptions to that length by adding padding ","metadata":{}},{"cell_type":"code","source":"all_desc = list()\nfor key in train_descriptions.keys():\n    [all_desc.append(d) for d in train_descriptions[key]]\nlines = all_desc\nmax_length = max(len(d.split()) for d in lines)\n\nprint('Description Length: %d' % max_length)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T07:08:22.676739Z","iopub.execute_input":"2022-05-08T07:08:22.677257Z","iopub.status.idle":"2022-05-08T07:08:22.718116Z","shell.execute_reply.started":"2022-05-08T07:08:22.677215Z","shell.execute_reply":"2022-05-08T07:08:22.716543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Word vectors map words to a vector space.\n#### In the vector space similar words are clustered and different words are separated\n#### The basic premise behind Glove is that we can derive sematic relationships between words from the concurrence matrix.","metadata":{}},{"cell_type":"code","source":"embeddings_index = {} \nf = open(os.path.join(glove_path, 'glove.6B.200d.txt'), encoding=\"utf-8\")\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs","metadata":{"execution":{"iopub.status.busy":"2022-05-08T07:08:22.719644Z","iopub.execute_input":"2022-05-08T07:08:22.72008Z","iopub.status.idle":"2022-05-08T07:08:45.862275Z","shell.execute_reply.started":"2022-05-08T07:08:22.720042Z","shell.execute_reply":"2022-05-08T07:08:45.861418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Here we make a concurrence matrix of shape(1660, 220) consisting of our vocabulary and the 200-d vector","metadata":{}},{"cell_type":"code","source":"embedding_dim = 200\nembedding_matrix = np.zeros((vocab_size, embedding_dim))\nfor word, i in word_to_idx.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","metadata":{"execution":{"iopub.status.busy":"2022-05-08T07:08:45.863632Z","iopub.execute_input":"2022-05-08T07:08:45.8639Z","iopub.status.idle":"2022-05-08T07:08:45.87597Z","shell.execute_reply.started":"2022-05-08T07:08:45.863867Z","shell.execute_reply":"2022-05-08T07:08:45.875102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### We are using InceptionV3 model which is pretrained on Imagenet dataset","metadata":{}},{"cell_type":"code","source":"model = InceptionV3(weights='imagenet')","metadata":{"execution":{"iopub.status.busy":"2022-05-08T07:08:45.877631Z","iopub.execute_input":"2022-05-08T07:08:45.878258Z","iopub.status.idle":"2022-05-08T07:08:50.969206Z","shell.execute_reply.started":"2022-05-08T07:08:45.878218Z","shell.execute_reply":"2022-05-08T07:08:50.968318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_new = Model(model.input, model.layers[-2].output)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T07:08:50.972619Z","iopub.execute_input":"2022-05-08T07:08:50.972912Z","iopub.status.idle":"2022-05-08T07:08:51.011798Z","shell.execute_reply.started":"2022-05-08T07:08:50.972882Z","shell.execute_reply":"2022-05-08T07:08:51.010915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Preprocess the training data set by converting each images into shape (299, 299)","metadata":{}},{"cell_type":"code","source":"def preprocess(image_path):\n    img = image.load_img(image_path, target_size=(299, 299))\n    x = image.img_to_array(img)\n    x = np.expand_dims(x, axis=0)\n    x = preprocess_input(x)\n    return x","metadata":{"execution":{"iopub.status.busy":"2022-05-08T07:08:51.016306Z","iopub.execute_input":"2022-05-08T07:08:51.018295Z","iopub.status.idle":"2022-05-08T07:08:51.025512Z","shell.execute_reply.started":"2022-05-08T07:08:51.018255Z","shell.execute_reply":"2022-05-08T07:08:51.024552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Now  we can train and test images on our model","metadata":{}},{"cell_type":"code","source":"def encode_image(image):\n    image = preprocess(image) \n    fea_vec = model_new.predict(image) \n    fea_vec = np.reshape(fea_vec, fea_vec.shape[1])\n    return fea_vec\n\nencoding_train = {}\nfor img in train_img:\n    encoding_train[img[len(images_path):]] = encode_image(img)\ntrain_features = encoding_train\n\nencoding_test = {}\nfor img in test_img:\n    encoding_test[img[len(images_path):]] = encode_image(img)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T07:08:51.030113Z","iopub.execute_input":"2022-05-08T07:08:51.032631Z","iopub.status.idle":"2022-05-08T07:16:28.893476Z","shell.execute_reply.started":"2022-05-08T07:08:51.032578Z","shell.execute_reply":"2022-05-08T07:16:28.892633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Here we define our model\n#### First it preprocess the sequence from the text\n#### Then we extract feature vector from the images\n#### We feed the feature vector to the fully connected layers\n#### There are 2 FE layers\n#### Then at last we use softmax activation function to decode","metadata":{}},{"cell_type":"code","source":"inputs1 = Input(shape=(2048,))\nfe1 = Dropout(0.5)(inputs1)\nfe2 = Dense(256, activation='relu')(fe1)\n\ninputs2 = Input(shape=(max_length,))\nse1 = Embedding(vocab_size, embedding_dim, mask_zero=True)(inputs2)\nse2 = Dropout(0.5)(se1)\nse3 = LSTM(256)(se2)\n\ndecoder1 = add([fe2, se3])\ndecoder2 = Dense(256, activation='relu')(decoder1)\noutputs = Dense(vocab_size, activation='softmax')(decoder2)\n\nmodel = Model(inputs=[inputs1, inputs2], outputs=outputs)\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-05-08T07:16:28.895141Z","iopub.execute_input":"2022-05-08T07:16:28.895405Z","iopub.status.idle":"2022-05-08T07:16:30.041192Z","shell.execute_reply.started":"2022-05-08T07:16:28.895369Z","shell.execute_reply":"2022-05-08T07:16:30.040366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Train the model","metadata":{}},{"cell_type":"code","source":"model.layers[2].set_weights([embedding_matrix])\nmodel.layers[2].trainable = False","metadata":{"execution":{"iopub.status.busy":"2022-05-08T07:16:30.042728Z","iopub.execute_input":"2022-05-08T07:16:30.042986Z","iopub.status.idle":"2022-05-08T07:16:30.051656Z","shell.execute_reply.started":"2022-05-08T07:16:30.042949Z","shell.execute_reply":"2022-05-08T07:16:30.050886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Use Adam optimizer and categorical_crossentropy to calculate loss","metadata":{}},{"cell_type":"code","source":"model.compile(loss='categorical_crossentropy', optimizer='adam')","metadata":{"execution":{"iopub.status.busy":"2022-05-08T07:16:30.052793Z","iopub.execute_input":"2022-05-08T07:16:30.053042Z","iopub.status.idle":"2022-05-08T07:16:30.068371Z","shell.execute_reply.started":"2022-05-08T07:16:30.053003Z","shell.execute_reply":"2022-05-08T07:16:30.067549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Create batches of dataset to and train it batch wise","metadata":{}},{"cell_type":"code","source":"def data_generator(descriptions, photos, word_to_idx, max_length, num_photos_per_batch):\n    X1, X2, y = list(), list(), list()\n    n=0\n    while 1:\n        for key, desc_list in descriptions.items():\n            n+=1\n            photo = photos[key +'.jpg']\n            for desc in desc_list:\n                seq = [word_to_idx[word] for word in desc.split(' ') if word in word_to_idx]\n                for i in range(1, len(seq)):\n                    in_seq, out_seq = seq[:i], seq[i]\n                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n                    X1.append(photo)\n                    X2.append(in_seq)\n                    y.append(out_seq)\n\n            if n==num_photos_per_batch:\n                yield ([array(X1), array(X2)], array(y))\n                X1, X2, y = list(), list(), list()\n                n=0","metadata":{"execution":{"iopub.status.busy":"2022-05-08T07:16:30.069819Z","iopub.execute_input":"2022-05-08T07:16:30.070247Z","iopub.status.idle":"2022-05-08T07:16:30.081752Z","shell.execute_reply.started":"2022-05-08T07:16:30.0702Z","shell.execute_reply":"2022-05-08T07:16:30.080997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Train the image for 30 epochs and give batch size of 3","metadata":{}},{"cell_type":"code","source":"epochs = 30\nbatch_size = 3\nsteps = len(train_descriptions)//batch_size\n\ngenerator = data_generator(train_descriptions, train_features, word_to_idx, max_length, batch_size)\nmodel.fit(generator, epochs=epochs, steps_per_epoch=steps, verbose=1)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T07:16:30.083193Z","iopub.execute_input":"2022-05-08T07:16:30.083536Z","iopub.status.idle":"2022-05-08T09:08:52.811987Z","shell.execute_reply.started":"2022-05-08T07:16:30.083497Z","shell.execute_reply":"2022-05-08T09:08:52.81117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### We use beam search to get the best caption predicted","metadata":{}},{"cell_type":"code","source":"def beam_search(image, beam_index = 3):\n    start = [word_to_idx[\"startseq\"]]\n    start_word = [[start, 0.0]]\n    while len(start_word[0][0]) < max_length:\n        temp = []\n        for s in start_word:\n            par_caps = sequence.pad_sequences([s[0]], maxlen=max_length, padding='post')\n            preds = model.predict([image,par_caps], verbose=0)\n            word_preds = np.argsort(preds[0])[-beam_index:]\n            for w in word_preds:\n                next_cap, prob = s[0][:], s[1]\n                next_cap.append(w)\n                prob += preds[0][w]\n                temp.append([next_cap, prob])\n                    \n        start_word = temp\n        start_word = sorted(start_word, reverse=False, key=lambda l: l[1])\n        start_word = start_word[-beam_index:]\n    \n    start_word = start_word[-1][0]\n    intermediate_caption = [idx_to_word[i] for i in start_word]\n    final_caption = []\n    \n    for i in intermediate_caption:\n        if i != 'endseq':\n            final_caption.append(i)\n        else:\n            break\n\n    final_caption = ' '.join(final_caption[1:])\n    return final_caption","metadata":{"execution":{"iopub.status.busy":"2022-05-08T09:08:52.815657Z","iopub.execute_input":"2022-05-08T09:08:52.816333Z","iopub.status.idle":"2022-05-08T09:08:52.826412Z","shell.execute_reply.started":"2022-05-08T09:08:52.816294Z","shell.execute_reply":"2022-05-08T09:08:52.825716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### We observed that when we did beam search with beam index = 7","metadata":{}},{"cell_type":"code","source":"pic = '2398605966_1d0c9e6a20.jpg'\nimage = encoding_test[pic].reshape((1,2048))\nx=plt.imread(images_path + pic)\nplt.imshow(x)\nplt.show()\n\nprint(\"Beam Search, K = 3:\",beam_search(image, beam_index = 3))\nprint(\"Beam Search, K = 5:\",beam_search(image, beam_index = 5))\nprint(\"Beam Search, K = 7:\",beam_search(image, beam_index = 7))\nprint(\"Beam Search, K = 10:\",beam_search(image, beam_index = 10))","metadata":{"execution":{"iopub.status.busy":"2022-05-08T09:08:52.828205Z","iopub.execute_input":"2022-05-08T09:08:52.828467Z","iopub.status.idle":"2022-05-08T09:09:31.704128Z","shell.execute_reply.started":"2022-05-08T09:08:52.828434Z","shell.execute_reply":"2022-05-08T09:09:31.703345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(test_descriptions['2398605966_1d0c9e6a20'])","metadata":{"execution":{"iopub.status.busy":"2022-05-08T09:11:14.207611Z","iopub.execute_input":"2022-05-08T09:11:14.208148Z","iopub.status.idle":"2022-05-08T09:11:14.21301Z","shell.execute_reply.started":"2022-05-08T09:11:14.208111Z","shell.execute_reply":"2022-05-08T09:11:14.212104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"captions = []\nfor caption in test_descriptions['2398605966_1d0c9e6a20']:\n    captions.append(caption.split())","metadata":{"execution":{"iopub.status.busy":"2022-05-08T09:13:02.86123Z","iopub.execute_input":"2022-05-08T09:13:02.862002Z","iopub.status.idle":"2022-05-08T09:13:02.867082Z","shell.execute_reply.started":"2022-05-08T09:13:02.861957Z","shell.execute_reply":"2022-05-08T09:13:02.866399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.translate.bleu_score import sentence_bleu\nsentence_bleu(captions, beam_search(image, beam_index = 7).split(' '))\n","metadata":{"execution":{"iopub.status.busy":"2022-05-08T09:17:24.482506Z","iopub.execute_input":"2022-05-08T09:17:24.482817Z","iopub.status.idle":"2022-05-08T09:17:34.780193Z","shell.execute_reply.started":"2022-05-08T09:17:24.482784Z","shell.execute_reply":"2022-05-08T09:17:34.779532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Predict the bleu_score for 200 images and take the average\n#### We are getting an average around 50 percent. ","metadata":{}},{"cell_type":"code","source":"bleu_score = 0;\ni=1\nfor key, caption_list in test_descriptions.items():\n    captions = []\n    if i==100:\n        break\n    for caption in caption_list:\n        captions.append(caption.split())\n    score = sentence_bleu(captions, beam_search(encoding_test[key + '.jpg'].reshape((1,2048)), beam_index = 7).split(' '))\n    bleu_score +=score\n    if i%10==0:\n        print(bleu_score/i)\n    i+=1\n        ","metadata":{"execution":{"iopub.status.busy":"2022-05-08T09:59:44.171615Z","iopub.execute_input":"2022-05-08T09:59:44.172323Z","iopub.status.idle":"2022-05-08T10:16:44.811319Z","shell.execute_reply.started":"2022-05-08T09:59:44.172286Z","shell.execute_reply":"2022-05-08T10:16:44.810342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(bleu_score/100)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T10:21:10.549479Z","iopub.execute_input":"2022-05-08T10:21:10.549761Z","iopub.status.idle":"2022-05-08T10:21:10.555313Z","shell.execute_reply.started":"2022-05-08T10:21:10.54973Z","shell.execute_reply":"2022-05-08T10:21:10.554575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### To observe the bias we checked the bleu_score of all the test images and\n#### If any image has score < 0.25 we check what is the caption predict","metadata":{}},{"cell_type":"code","source":"i=1\nedge_cases = []\nfor key, caption_list in test_descriptions.items():\n    captions = []\n    if i==100:\n        break\n    for caption in caption_list:\n        captions.append(caption.split())\n    score = sentence_bleu(captions, beam_search(encoding_test[key + '.jpg'].reshape((1,2048)), beam_index = 7).split(' '))\n    if i%10==0:\n        print(score)\n    if score < 0.25:\n        print(score)\n        edge_cases.append(key)\n        edge_cases.append(score)\n    i+=1","metadata":{"execution":{"iopub.status.busy":"2022-05-08T10:23:39.168504Z","iopub.execute_input":"2022-05-08T10:23:39.16914Z","iopub.status.idle":"2022-05-08T10:40:41.044931Z","shell.execute_reply.started":"2022-05-08T10:23:39.169103Z","shell.execute_reply":"2022-05-08T10:40:41.044081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(edge_cases)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-08T10:48:04.152152Z","iopub.execute_input":"2022-05-08T10:48:04.152751Z","iopub.status.idle":"2022-05-08T10:48:04.157009Z","shell.execute_reply.started":"2022-05-08T10:48:04.152715Z","shell.execute_reply":"2022-05-08T10:48:04.156123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pic = '1554713437_61b64527dd.jpg'\nimage = encoding_test[pic].reshape((1,2048))\nx=plt.imread(images_path + pic)\nplt.imshow(x)\nplt.show()\nprint(test_descriptions['1554713437_61b64527dd'])\nprint(\"Beam Search, K = 7:\",beam_search(image, beam_index = 7))\n","metadata":{"execution":{"iopub.status.busy":"2022-05-08T10:49:48.009309Z","iopub.execute_input":"2022-05-08T10:49:48.009579Z","iopub.status.idle":"2022-05-08T10:49:58.157208Z","shell.execute_reply.started":"2022-05-08T10:49:48.009549Z","shell.execute_reply":"2022-05-08T10:49:58.156364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pic = '1679617928_a73c1769be.jpg'\nimage = encoding_test[pic].reshape((1,2048))\nx=plt.imread(images_path + pic)\nplt.imshow(x)\nplt.show()\nprint(test_descriptions['1679617928_a73c1769be'])\nprint(\"Beam Search, K = 7:\",beam_search(image, beam_index = 7))","metadata":{"execution":{"iopub.status.busy":"2022-05-08T10:51:20.002001Z","iopub.execute_input":"2022-05-08T10:51:20.002291Z","iopub.status.idle":"2022-05-08T10:51:30.777701Z","shell.execute_reply.started":"2022-05-08T10:51:20.002263Z","shell.execute_reply":"2022-05-08T10:51:30.776885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pic = '180094434_b0f244832d.jpg'\nimage = encoding_test[pic].reshape((1,2048))\nx=plt.imread(images_path + pic)\nplt.imshow(x)\nplt.show()\nprint(test_descriptions['180094434_b0f244832d'])\nprint(\"Beam Search, K = 7:\",beam_search(image, beam_index = 7))","metadata":{"execution":{"iopub.status.busy":"2022-05-08T10:52:36.56897Z","iopub.execute_input":"2022-05-08T10:52:36.569274Z","iopub.status.idle":"2022-05-08T10:52:47.509082Z","shell.execute_reply.started":"2022-05-08T10:52:36.569241Z","shell.execute_reply":"2022-05-08T10:52:47.508345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"i=0\nedge_cases = []\nfor key, caption_list in test_descriptions.items():\n    captions = []\n    i+=1\n    if i<101:\n        continue\n    if i==200:\n        break\n    for caption in caption_list:\n        captions.append(caption.split())\n    score = sentence_bleu(captions, beam_search(encoding_test[key + '.jpg'].reshape((1,2048)), beam_index = 7).split(' '))\n    if i%10==0:#\n        print(score)\n    if score < 0.25:\n        print(score)\n        edge_cases.append(key)\n        edge_cases.append(score)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-08T10:58:18.896353Z","iopub.execute_input":"2022-05-08T10:58:18.896626Z","iopub.status.idle":"2022-05-08T11:15:21.303529Z","shell.execute_reply.started":"2022-05-08T10:58:18.896597Z","shell.execute_reply":"2022-05-08T11:15:21.302732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(edge_cases)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T11:15:39.350325Z","iopub.execute_input":"2022-05-08T11:15:39.350594Z","iopub.status.idle":"2022-05-08T11:15:39.355235Z","shell.execute_reply.started":"2022-05-08T11:15:39.350564Z","shell.execute_reply":"2022-05-08T11:15:39.354111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Give the predicted captions for a particular image ","metadata":{}},{"cell_type":"code","source":"def predict(name):\n    pic = name + '.jpg'\n    image = encoding_test[pic].reshape((1,2048))\n    x=plt.imread(images_path + pic)\n    plt.imshow(x)\n    plt.show()\n    print(test_descriptions[name])\n    print(\"Beam Search, K = 7:\",beam_search(image, beam_index = 7))","metadata":{"execution":{"iopub.status.busy":"2022-05-08T11:17:21.727256Z","iopub.execute_input":"2022-05-08T11:17:21.727533Z","iopub.status.idle":"2022-05-08T11:17:21.733715Z","shell.execute_reply.started":"2022-05-08T11:17:21.727503Z","shell.execute_reply":"2022-05-08T11:17:21.732797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### These are some of the examples of the images we found which are biased","metadata":{}},{"cell_type":"code","source":"predict('191003285_edd8d0cf58')\npredict('197504190_fd1fc3d4b7')\npredict('2248487950_c62d0c81a9')\npredict('2309860995_c2e2a0feeb')\npredict('2176980976_7054c99621')\npredict('2196107384_361d73a170')\npredict('2274992140_bb9e868bb8')","metadata":{"execution":{"iopub.status.busy":"2022-05-08T11:17:24.559398Z","iopub.execute_input":"2022-05-08T11:17:24.559662Z","iopub.status.idle":"2022-05-08T11:17:34.853586Z","shell.execute_reply.started":"2022-05-08T11:17:24.559632Z","shell.execute_reply":"2022-05-08T11:17:34.852847Z"},"trusted":true},"execution_count":null,"outputs":[]}]}